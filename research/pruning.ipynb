{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ubatch = torch.rand(8, 197, 768)\n",
    "ubatch_labels = torch.tensor([3, 13, 15, 20, 40, 285, 285, 285])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nn.Linear(768, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2766,  0.0448,  0.0932,  ..., -0.0393,  0.5249, -0.0993],\n",
       "         [-0.0343, -0.2425,  0.1282,  ...,  0.3778,  0.2658,  0.2387],\n",
       "         [ 0.1523, -0.4120,  0.0839,  ..., -0.1985,  0.1147, -0.2054],\n",
       "         ...,\n",
       "         [ 0.0630, -0.1931,  0.0755,  ...,  0.0412,  0.1973, -0.1324],\n",
       "         [ 0.1684, -0.3691,  0.1220,  ...,  0.1200,  0.3867,  0.0300],\n",
       "         [ 0.0945, -0.2799,  0.2160,  ...,  0.3509,  0.5158, -0.1098]],\n",
       "\n",
       "        [[-0.0284, -0.1280, -0.1112,  ...,  0.3189,  0.0875,  0.1434],\n",
       "         [-0.3748, -0.3916,  0.0476,  ...,  0.0858,  0.1880, -0.0721],\n",
       "         [ 0.0698, -0.4706,  0.1800,  ..., -0.0121,  0.3323, -0.3474],\n",
       "         ...,\n",
       "         [-0.2075, -0.4918,  0.1257,  ...,  0.3077,  0.5018, -0.2906],\n",
       "         [ 0.0824, -0.4345,  0.1456,  ..., -0.0886,  0.5073,  0.3248],\n",
       "         [ 0.0635, -0.2664,  0.2375,  ...,  0.2204,  0.3783,  0.1531]],\n",
       "\n",
       "        [[ 0.1382, -0.2284,  0.0519,  ..., -0.1145,  0.5667,  0.0708],\n",
       "         [ 0.3807, -0.1844,  0.1266,  ..., -0.1578,  0.1582,  0.1857],\n",
       "         [-0.0671, -0.3187,  0.1202,  ...,  0.2127,  0.3081, -0.1229],\n",
       "         ...,\n",
       "         [-0.0477, -0.6289,  0.2727,  ..., -0.0522,  0.6873, -0.1403],\n",
       "         [ 0.2063, -0.2397, -0.2055,  ..., -0.1635,  0.1573,  0.1033],\n",
       "         [-0.1215, -0.1361,  0.1077,  ...,  0.0164,  0.4505, -0.2297]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 0.0792, -0.0613,  0.0134,  ...,  0.1699,  0.0875, -0.1395],\n",
       "         [ 0.2888,  0.0161,  0.2224,  ..., -0.0874,  0.5392, -0.2119],\n",
       "         [ 0.4600, -0.6410,  0.6456,  ...,  0.2904,  0.5927, -0.0871],\n",
       "         ...,\n",
       "         [-0.0946, -0.5604,  0.1439,  ..., -0.1800,  0.4493, -0.3672],\n",
       "         [-0.0591, -0.2676,  0.1866,  ...,  0.3601,  0.4715, -0.5005],\n",
       "         [ 0.0196, -0.2616, -0.0136,  ...,  0.1931,  0.3234, -0.2175]],\n",
       "\n",
       "        [[-0.1080, -0.4761,  0.1753,  ...,  0.4336,  0.4176, -0.0997],\n",
       "         [ 0.2461, -0.4737,  0.2189,  ..., -0.2033,  0.6097,  0.0218],\n",
       "         [ 0.1182, -0.0568,  0.3783,  ...,  0.0971,  0.3324,  0.1267],\n",
       "         ...,\n",
       "         [ 0.1398, -0.3756,  0.4494,  ...,  0.3231,  0.2154,  0.2714],\n",
       "         [ 0.0980, -0.3286, -0.0431,  ..., -0.0180,  0.1612, -0.2846],\n",
       "         [ 0.1049, -0.4713, -0.0124,  ..., -0.1081,  0.3862, -0.0900]],\n",
       "\n",
       "        [[-0.1112, -0.2638,  0.2163,  ..., -0.2373,  0.3876, -0.0820],\n",
       "         [ 0.4656, -0.1460,  0.3092,  ..., -0.1161,  0.3416,  0.0408],\n",
       "         [ 0.0670, -0.0863,  0.0056,  ..., -0.1597, -0.0059, -0.1748],\n",
       "         ...,\n",
       "         [ 0.1738, -0.2094,  0.1971,  ...,  0.4567,  0.2719, -0.2693],\n",
       "         [ 0.3463, -0.2309,  0.2881,  ...,  0.3309,  0.3045,  0.0108],\n",
       "         [ 0.0426, -0.4592,  0.2702,  ...,  0.1647,  0.3644, -0.1482]]],\n",
       "       grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier(ubatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleShard(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "class VitImageClassification(ModuleShard):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.classifier = nn.Linear(768, 1000)\n",
    "    \n",
    "def snip_forward_linear(self, x):\n",
    "    return F.linear(x, self.weight * self.weight_mask, self.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit = VitImageClassification()\n",
    "vit.classifier.weight_mask = nn.Parameter(torch.ones_like(vit.classifier.weight))\n",
    "vit.classifier.forward = types.MethodType(snip_forward_linear, vit.classifier)\n",
    "vit.zero_grad()\n",
    "output = vit.classifier(ubatch[:,0,:])\n",
    "loss = F.nll_loss(output, ubatch_labels)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(76)\n"
     ]
    }
   ],
   "source": [
    "grads_abs = []\n",
    "grads_abs.append(torch.abs(vit.classifier.weight_mask.grad))\n",
    "# grads_abs.append(torch.abs(a))\n",
    "all_scores = torch.cat([torch.flatten(x) for x in grads_abs])\n",
    "norm_factor = torch.sum(all_scores)\n",
    "all_scores.div_(norm_factor)\n",
    "keep_ratio = 0.0001\n",
    "num_params_to_keep = int(len(all_scores) * keep_ratio)\n",
    "threshold, _ = torch.topk(all_scores, num_params_to_keep, sorted=True)\n",
    "acceptable_score = threshold[-1]\n",
    "keep_masks = []\n",
    "for g in grads_abs:\n",
    "    keep_masks.append(((g / norm_factor) >= acceptable_score).float())\n",
    "print(torch.sum(torch.cat([torch.flatten(x == 1) for x in keep_masks])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "grads_abs = []\n",
    "grads_abs.append(torch.abs(vit.classifier.weight_mask.grad))\n",
    "grads_abs.append(torch.abs(vit.classifier.weight_mask.grad))\n",
    "all_scores = torch.cat([torch.flatten(x) for x in grads_abs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "        1., 1., 1., 1.])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep_masks[0][keep_masks[0] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([767924])"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.classifier.weight.data[keep_masks[0] == 0.].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]),)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[164], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m keep_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keep_masks):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(keep_mask)\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mkeep_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "for keep_mask in zip(keep_masks):\n",
    "    print(keep_mask)\n",
    "    keep_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "vit.classifier.weight.data[keep_masks[0] == 0.] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(768000)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pipellm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
